# Nightingale AI - Technical Brief

**Version:** 1.0  
**Date:** February 2, 2026  
**Author:** Engineering Team

---

## 1. Architecture Overview

### 1.1 System Design

Nightingale AI follows a three-tier architecture consisting of a React single-page application, a Node.js REST API, and a PostgreSQL database. The frontend communicates with the backend exclusively over HTTPS using JWT bearer tokens for authentication. All sensitive data is encrypted at rest using AES-256-GCM.

The backend is built with Express.js and TypeScript, organized into distinct layers: routes define API endpoints, middleware handles authentication and authorization, services contain business logic (including AI interactions), and models manage database operations. This separation ensures testability and maintainability.

External AI capabilities are provided by Groq's inference API. We use Llama 3.3-70b-versatile for conversational responses, risk assessment, and fact extraction. For future voice features, Groq's Whisper-large-v3 handles speech-to-text transcription.

### 1.2 Request Processing Pipeline

When a patient sends a message, the system processes it through five sequential stages. First, the authentication middleware validates the JWT token and verifies the session exists in the database. Second, role-based access control confirms the user has permission to access the chat endpoint (patients only).

Third, the PHI redaction pipeline scans the message for personally identifiable information. It uses the patient's known names from their profile and regex patterns for ID numbers (Singapore NRIC, US SSN) and phone numbers. These are replaced with placeholders like [PERSON_1] or [ID_NUMBER_1] before any data leaves our system boundary.

Fourth, the redacted message is sent to Groq for three parallel operations: risk assessment determines urgency (low/medium/high) using a deterministic prompt with temperature 0.1; fact extraction identifies symptoms, medications, allergies, and conditions; and the conversational response provides an empathetic reply with citations to the patient's profile or previous statements.

Finally, the system persists all data: the patient message is encrypted and stored with its risk assessment, the AI response is stored with confidence scores and citations, extracted facts update the patient's living memory with provenance links, and if the risk level is high, an escalation record is created for the clinic's triage queue.

---

## 2. Data Schema and Relationships

### 2.1 Core Entities

The database consists of seven primary tables. The **users** table stores both patients and clinicians, distinguished by a role field that drives access control throughout the system. Each user belongs to a **clinic**, which scopes clinician access to only their assigned patients.

The **conversations** table represents a chat session between a patient and the AI. Each conversation has a status (active, escalated, or closed) and contains multiple **messages**. Messages store the encrypted content along with metadata: sender type (patient, AI, or clinician), risk assessment fields for patient messages, and confidence/citation fields for AI responses.

### 2.2 Living Memory Model

The **patient_memory** table implements our Living Memory feature. Each row represents a single fact about the patient: their symptoms, medications, allergies, conditions, or lifestyle factors. The memory_type field categorizes the fact, while the status field tracks its current state (active, stopped, resolved, or corrected).

Critically, every memory item includes a provenance_message_id that links back to the exact message where the fact was first mentioned. This creates an auditable chain: clinicians can see not just what the patient reported, but when they said it and in what context. When patients correct themselves ("Actually, I stopped taking Advil"), the system updates the status and records the mutation timestamp in updated_at.

### 2.3 Escalation Workflow

The **escalations** table bridges the AI system and human clinicians. When risk assessment determines a message requires human attention, an escalation record is created containing: a reference to the triggering conversation and message, a triage_summary (1-5 bullet points generated by the AI summarizing the situation), and a profile_snapshot capturing the patient's memory at that exact moment.

The profile_snapshot is essential because patient memory can change. A clinician reviewing an escalation needs to see what the AI knew at the time of escalation, not the patient's current profile which may have been updated by subsequent conversations.

### 2.4 How Components Link Together

Messages form the backbone of the system. A patient message triggers fact extraction, which creates or updates patient_memory records with provenance links back to that message. The AI response includes citations (stored as JSONB) referencing either "their health profile" (patient_memory) or "their previous message" (earlier messages in the conversation).

When escalation occurs, the escalation record captures the triggering_message_id, the conversation_id for context, and snapshots both the triage summary and patient profile. Clinicians can then respond directly in the conversation, and their messages are stored with sender_type "clinician" and mapped to the system role when building context for future AI responses—ensuring clinician guidance takes precedence over AI suggestions.

---

## 3. Assumptions and First-Principles Thinking

### 3.1 Security Assumptions

We assume that any data sent to external services (including Groq) could potentially be compromised. Therefore, PHI redaction is non-negotiable—patient names, government IDs, and phone numbers are stripped before any API call. Even if Groq's privacy policy claims data is not retained, we operate under a zero-trust model.

We also assume the database could be breached. All message content is encrypted with AES-256-GCM using a key stored in environment variables, never in code or database. Audit logs use SHA-256 hashing for all identifiers, allowing correlation analysis without exposing PHI.

### 3.2 Clinical Safety Assumptions

Risk assessment errs on the side of caution. If the AI cannot confidently assess risk, it defaults to "medium" rather than "low." False positives (unnecessary escalations) are preferable to false negatives (missed emergencies). The system prompt explicitly instructs the AI to recommend emergency services for any mention of chest pain, breathing difficulty, or self-harm ideation.

We assume clinician input represents ground truth. When a clinician sends a message in an escalated conversation, that message is mapped to the "system" role with a "CLINICIAN GUIDANCE:" prefix when building context for subsequent AI responses. This ensures the AI defers to medical professional judgment rather than contradicting it.

### 3.3 User Behavior Assumptions

Patients provide incomplete and sometimes contradictory information. The living memory system accounts for this by supporting status mutations. A patient who says "I take Advil" and later says "I stopped taking Advil" doesn't create two conflicting records—the original record is updated with status "stopped" and a new timeline note.

We also assume patients may not use medical terminology correctly. The fact extraction prompt is designed to normalize common variations ("headache" vs "head pain" vs "my head hurts") into consistent memory entries.

---

## 4. Trade-offs and Scope Decisions

### 4.1 What We Built

The core product delivers a complete patient intake workflow: conversational AI that gathers health information, assesses risk in real-time, maintains an evolving patient profile, and escalates to human clinicians when appropriate. The clinic-side interface allows clinicians to view their triage queue, see AI-generated summaries, review patient history, and respond directly to patients.

We implemented full RBAC at three layers: frontend route guards prevent UI access, middleware blocks unauthorized API calls, and database queries filter by user/clinic ownership. PHI redaction covers names (using patient profile data), government IDs (Singapore NRIC, US SSN patterns), and phone numbers (international formats).

### 4.2 What We Cut

**Real-time messaging via WebSockets** was descoped in favor of HTTP polling. Healthcare conversations don't require instant delivery—patients expect thoughtful responses, not chat-like immediacy. This significantly reduced complexity.

**Multi-language support** was deferred. Internationalization requires not just UI translation but also adapting the AI prompts, fact extraction patterns, and risk assessment logic for different cultural contexts. English-only for the demo.

**File and image attachments** were excluded. Supporting attachments requires separate object storage (S3/Azure Blob), virus scanning, image compression, and HIPAA-compliant access controls. The complexity exceeds the demo scope.

**Integration with external systems** (EHR, appointment scheduling, email/SMS notifications) was mocked. Real integrations require vendor partnerships, API credentials, and compliance agreements that can't be established for a demo.

### 4.3 Technical Debt

We acknowledge several areas requiring production hardening. Session management currently uses in-database storage; production should use Redis for performance and distributed deployment. LLM calls are synchronous and blocking; a message queue would improve resilience. Rate limiting is not implemented; production requires Redis-backed throttling to prevent abuse.

The test suite requires a running backend, which complicates CI/CD. Unit tests with mocked dependencies would improve the development workflow.

---

## 5. Voice AI Strategy

### 5.1 Current State

The database schema is voice-ready. The messages table includes four fields for audio support: audio_id (reference to stored audio file), audio_transcript (text output from speech-to-text), audio_duration_seconds (for analytics and billing), and is_voice_message (boolean flag to distinguish voice from text input).

The backend includes stubbed voice routes at /api/voice with endpoints for transcription and synthesis. These currently integrate with Groq Whisper for speech-to-text and Google TTS for text-to-speech, but are not wired into the main chat flow.

### 5.2 Integration Approach

Voice input would follow this flow: the frontend captures audio using the MediaRecorder API, uploads it to the /api/voice/chat endpoint, which transcribes the audio using Whisper, then processes the transcript through the existing text pipeline (redaction, risk assessment, fact extraction, response generation). The AI response text would then be synthesized to audio using TTS and returned alongside the transcript.

The key insight is that voice doesn't require a separate processing pipeline—it's a different input/output modality feeding the same core logic. The transcript becomes the message content, subject to the same redaction and analysis as typed text.

### 5.3 Storage Considerations

Two strategies exist for audio storage. The first stores audio blobs in object storage (S3 with server-side encryption), keeping the audio_id reference in the database. This enables playback and provides a complete audit trail but requires HIPAA-compliant storage configuration and increases costs.

The second strategy discards audio after transcription, storing only the transcript. This simplifies compliance (text encryption is already implemented) and reduces storage costs, but loses the ability to replay or verify transcriptions. Given that transcription errors are possible, the first approach is recommended for production.

### 5.4 PHI in Voice

Spoken PHI presents a unique challenge. Patients may say their name and ID number aloud in a voice message. The solution is straightforward: transcribe first (Whisper doesn't need PHI removal), then apply the same redaction pipeline to the transcript before sending to the conversational LLM. The original audio, if stored, should be encrypted with the same key as message content.

---

## 6. Conclusion

Nightingale AI demonstrates that healthcare-compliant conversational AI is achievable with careful architecture. By treating PHI protection as a first-class concern, implementing conservative risk assessment, and designing for human oversight, the system provides value to patients while maintaining the safety guardrails healthcare demands.

The architecture separates concerns cleanly: the AI handles conversation and analysis, the database maintains encrypted state with full provenance, and humans retain ultimate authority through the escalation system. Voice capabilities can be added incrementally because the core pipeline is modality-agnostic.

Future work should focus on production hardening (Redis, message queues, proper rate limiting), expanded PHI detection (addresses, dates of birth, medical record numbers), and deeper EHR integration to provide clinicians with complete patient context.

---

*End of Technical Brief*
